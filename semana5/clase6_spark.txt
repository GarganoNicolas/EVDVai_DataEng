nico@home:~$ sudo docker exec -it edvai-hadoop bash
Error response from daemon: No such container: edvai-hadoop
nico@home:~$ sudo docker exec -it edvai_hadoop bash
root@7e7bdeaf0621:/# su hadoop
hadoop@7e7bdeaf0621:/$ pyspark
Python 3.8.10 (default, Mar 15 2022, 12:22:08) 
[GCC 9.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hadoop/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
2024-05-07 08:32:05,904 WARN conf.HiveConf: HiveConf of name hive.metastore.local does not exist
2024-05-07 08:32:06,243 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-05-07 08:32:13,407 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0
      /_/

Using Python version 3.8.10 (default, Mar 15 2022 12:22:08)
Spark context Web UI available at http://7e7bdeaf0621:4040
Spark context available as 'sc' (master = yarn, app id = application_1715081085847_0001).
SparkSession available as 'spark'.
>>> df = spark.read.option("header", "true").csv("/ingest/yellow_tripdata_2021-01.csv")

>>>                                                                             
>>> df.printSchema()
root
 |-- VendorID: string (nullable = true)
 |-- tpep_pickup_datetime: string (nullable = true)
 |-- tpep_dropoff_datetime: string (nullable = true)
 |-- passenger_count: string (nullable = true)
 |-- trip_distance: string (nullable = true)
 |-- RatecodeID: string (nullable = true)
 |-- store_and_fwd_flag: string (nullable = true)
 |-- PULocationID: string (nullable = true)
 |-- DOLocationID: string (nullable = true)
 |-- payment_type: string (nullable = true)
 |-- fare_amount: string (nullable = true)
 |-- extra: string (nullable = true)
 |-- mta_tax: string (nullable = true)
 |-- tip_amount: string (nullable = true)
 |-- tolls_amount: string (nullable = true)
 |-- improvement_surcharge: string (nullable = true)
 |-- total_amount: string (nullable = true)
 |-- congestion_surcharge: string (nullable = true)

>>> df.createOrReplaceTempView("v_df")
>>> df_cast = spark.sql("select cast(tpep_pickup_datetime as date), cast(passenger_count as int), cast(trip_distance as float) from v_df")
>>> df_cast.printSchema()
root
 |-- tpep_pickup_datetime: date (nullable = true)
 |-- passenger_count: integer (nullable = true)
 |-- trip_distance: float (nullable = true)

>>> df_cast.count()
1369765                                                                         
>>> df_cast.createTempView("v_cast")
>>> df_transform = spark.sql("Select * from v_cast where passenger_count > 1 and trip_distance > 10")
>>> df_transform.show(5)
+--------------------+---------------+-------------+                            
|tpep_pickup_datetime|passenger_count|trip_distance|
+--------------------+---------------+-------------+
|          2021-01-01|              2|         19.1|
|          2021-01-01|              3|        10.74|
|          2021-01-01|              3|        16.54|
|          2021-01-01|              2|         11.1|
|          2021-01-01|              2|        15.19|
+--------------------+---------------+-------------+
only showing top 5 rows

>>> df_transfrom.createTempView("v_insert")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'df_transfrom' is not defined
>>> df_transform.createTempView("v_insert")
>>> spark.sql("insert into tripsdb.tips select * from v_insert")
2024-05-07 09:59:04,974 WARN conf.HiveConf: HiveConf of name hive.metastore.local does not exist
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/hadoop/spark/python/pyspark/sql/session.py", line 723, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
  File "/home/hadoop/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py", line 1309, in __call__
  File "/home/hadoop/spark/python/pyspark/sql/utils.py", line 117, in deco
    raise converted from None
pyspark.sql.utils.AnalysisException: Table not found: tripsdb.tips; line 1 pos 12;
'InsertIntoStatement 'UnresolvedRelation [tripsdb, tips], [], false, false, false
+- Project [tpep_pickup_datetime#54, passenger_count#52, trip_distance#53]
   +- SubqueryAlias v_insert
      +- View (`v_insert`, [tpep_pickup_datetime#54,passenger_count#52,trip_distance#53])
         +- Project [tpep_pickup_datetime#54, passenger_count#52, trip_distance#53]
            +- Filter ((passenger_count#52 > 1) AND (trip_distance#53 > cast(10 as float)))
               +- SubqueryAlias v_cast
                  +- View (`v_cast`, [tpep_pickup_datetime#54,passenger_count#52,trip_distance#53])
                     +- Project [cast(tpep_pickup_datetime#17 as date) AS tpep_pickup_datetime#54, cast(passenger_count#19 as int) AS passenger_count#52, cast(trip_distance#20 as float) AS trip_distance#53]
                        +- SubqueryAlias v_df
                           +- View (`v_df`, [VendorID#16,tpep_pickup_datetime#17,tpep_dropoff_datetime#18,passenger_count#19,trip_distance#20,RatecodeID#21,store_and_fwd_flag#22,PULocationID#23,DOLocationID#24,payment_type#25,fare_amount#26,extra#27,mta_tax#28,tip_amount#29,tolls_amount#30,improvement_surcharge#31,total_amount#32,congestion_surcharge#33])
                              +- Relation [VendorID#16,tpep_pickup_datetime#17,tpep_dropoff_datetime#18,passenger_count#19,trip_distance#20,RatecodeID#21,store_and_fwd_flag#22,PULocationID#23,DOLocationID#24,payment_type#25,fare_amount#26,extra#27,mta_tax#28,tip_amount#29,tolls_amount#30,improvement_surcharge#31,total_amount#32,congestion_surcharge#33] csv

>>> spark.sql("insert into tripsdb.trips select * from v_insert")
2024-05-07 10:00:11,051 WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
DataFrame[]                                                                     
>>> py_df = spark.read.option("header", "true").csv("/ingest/yellow_tripdata_2021-01.csv")
>>> py_df_cast = py_df.select(df.tpep_pickup_datetime.cast("date"), py_df.passenger_count.cast("int"), df.trip_distance.cast("float"))
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/hadoop/spark/python/pyspark/sql/dataframe.py", line 1685, in select
    jdf = self._jdf.select(self._jcols(*cols))
  File "/home/hadoop/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py", line 1309, in __call__
  File "/home/hadoop/spark/python/pyspark/sql/utils.py", line 117, in deco
    raise converted from None
pyspark.sql.utils.AnalysisException: Resolved attribute(s) tpep_pickup_datetime#17,trip_distance#20 missing from VendorID#113,tpep_pickup_datetime#114,tpep_dropoff_datetime#115,passenger_count#116,trip_distance#117,RatecodeID#118,store_and_fwd_flag#119,PULocationID#120,DOLocationID#121,payment_type#122,fare_amount#123,extra#124,mta_tax#125,tip_amount#126,tolls_amount#127,improvement_surcharge#128,total_amount#129,congestion_surcharge#130 in operator !Project [cast(tpep_pickup_datetime#17 as date) AS tpep_pickup_datetime#151, cast(passenger_count#116 as int) AS passenger_count#149, cast(trip_distance#20 as float) AS trip_distance#150]. Attribute(s) with the same name appear in the operation: tpep_pickup_datetime,trip_distance. Please check if the right attribute(s) are used.;
!Project [cast(tpep_pickup_datetime#17 as date) AS tpep_pickup_datetime#151, cast(passenger_count#116 as int) AS passenger_count#149, cast(trip_distance#20 as float) AS trip_distance#150]
+- Relation [VendorID#113,tpep_pickup_datetime#114,tpep_dropoff_datetime#115,passenger_count#116,trip_distance#117,RatecodeID#118,store_and_fwd_flag#119,PULocationID#120,DOLocationID#121,payment_type#122,fare_amount#123,extra#124,mta_tax#125,tip_amount#126,tolls_amount#127,improvement_surcharge#128,total_amount#129,congestion_surcharge#130] csv

>>> py_df_cast = py_df.select(df.tpep_pickup_datetime.cast("date"), py_df.passenger_count.cast("int"), py_df.trip_distance.cast("float"))
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/hadoop/spark/python/pyspark/sql/dataframe.py", line 1685, in select
    jdf = self._jdf.select(self._jcols(*cols))
  File "/home/hadoop/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py", line 1309, in __call__
  File "/home/hadoop/spark/python/pyspark/sql/utils.py", line 117, in deco
    raise converted from None
pyspark.sql.utils.AnalysisException: Resolved attribute(s) tpep_pickup_datetime#17 missing from VendorID#113,tpep_pickup_datetime#114,tpep_dropoff_datetime#115,passenger_count#116,trip_distance#117,RatecodeID#118,store_and_fwd_flag#119,PULocationID#120,DOLocationID#121,payment_type#122,fare_amount#123,extra#124,mta_tax#125,tip_amount#126,tolls_amount#127,improvement_surcharge#128,total_amount#129,congestion_surcharge#130 in operator !Project [cast(tpep_pickup_datetime#17 as date) AS tpep_pickup_datetime#154, cast(passenger_count#116 as int) AS passenger_count#152, cast(trip_distance#117 as float) AS trip_distance#153]. Attribute(s) with the same name appear in the operation: tpep_pickup_datetime. Please check if the right attribute(s) are used.;
!Project [cast(tpep_pickup_datetime#17 as date) AS tpep_pickup_datetime#154, cast(passenger_count#116 as int) AS passenger_count#152, cast(trip_distance#117 as float) AS trip_distance#153]
+- Relation [VendorID#113,tpep_pickup_datetime#114,tpep_dropoff_datetime#115,passenger_count#116,trip_distance#117,RatecodeID#118,store_and_fwd_flag#119,PULocationID#120,DOLocationID#121,payment_type#122,fare_amount#123,extra#124,mta_tax#125,tip_amount#126,tolls_amount#127,improvement_surcharge#128,total_amount#129,congestion_surcharge#130] csv

>>> py_df_cast = py_df.select(py_df.tpep_pickup_datetime.cast("date"), py_df.passenger_count.cast("int"), py_df.trip_distance.cast("float"))
>>> df_cast.printSchema()
root
 |-- tpep_pickup_datetime: date (nullable = true)
 |-- passenger_count: integer (nullable = true)
 |-- trip_distance: float (nullable = true)

>>> py_df_cast.printSchema()
root
 |-- tpep_pickup_datetime: date (nullable = true)
 |-- passenger_count: integer (nullable = true)
 |-- trip_distance: float (nullable = true)

>>> #queria conservar el otro dataframe, pero al final me genero mas problemas que soluciones
>>> py_transform = py_df_cast.filter( (py_df_cast.passenger_count >1) & (pf_df_cast.trip_distance > 10) )
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'pf_df_cast' is not defined
>>> py_transform = py_df_cast.filter( (py_df_cast.passenger_count >1) & (py_df_cast.trip_distance > 10) )
>>> py_transform.show(7)
+--------------------+---------------+-------------+                            
|tpep_pickup_datetime|passenger_count|trip_distance|
+--------------------+---------------+-------------+
|          2021-01-01|              2|         19.1|
|          2021-01-01|              3|        10.74|
|          2021-01-01|              3|        16.54|
|          2021-01-01|              2|         11.1|
|          2021-01-01|              2|        15.19|
|          2021-01-01|              2|         15.4|
|          2021-01-01|              2|        17.68|
+--------------------+---------------+-------------+
only showing top 7 rows

>>> df_transform.write.insertInto("tripsdb.pytrips")
>>> # oh no! me equivoque de tabla!
>>> py_transform.write.insertInto("tripsdb.pytrips")
>>> exit()                                                                      
hadoop@7e7bdeaf0621:/$ exit
exit
root@7e7bdeaf0621:/# exit
exit
